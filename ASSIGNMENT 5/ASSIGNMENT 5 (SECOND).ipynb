{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60bf422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a46bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a5d1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path='train.csv', test_path='test.csv'):\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99d5cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, threshold=600):\n",
    "    initial_shape = df.shape\n",
    "    cols_to_drop = [col for col in df.columns if df[col].isnull().sum() > threshold]\n",
    "    if cols_to_drop:\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "        print(f\"Dropped columns with more than {threshold} null values: {cols_to_drop}\")\n",
    "    print(f\"Shape after dropping high-null columns: {df.shape}\")\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            skewness = df[col].skew()\n",
    "            fill_value = df[col].median() if abs(skewness) >= 0.5 else df[col].mean()\n",
    "            df[col] = df[col].fillna(fill_value)\n",
    "\n",
    "\n",
    "    categorical_cols = df.select_dtypes(include='object').columns\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().any():\n",
    "            mode_value = df[col].mode()[0] if not df[col].mode().empty else \"Unknown\"\n",
    "            df[col] = df[col].fillna(mode_value)\n",
    "\n",
    "    print(\"Missing values in numerical and categorical columns handled.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c97e8767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    if 'Id' in df.columns:\n",
    "        df = df.drop(columns=['Id'])\n",
    "        print(\"Dropped 'Id' column.\")\n",
    "\n",
    "    numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    if 'SalePrice' in numerical_features:\n",
    "        numerical_features.remove('SalePrice')\n",
    "\n",
    "    corr_matrix = df[numerical_features].corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop_high_corr = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
    "\n",
    "    if to_drop_high_corr:\n",
    "        df.drop(columns=to_drop_high_corr, inplace=True)\n",
    "        print(f\"Dropped highly correlated numerical features (corr > 0.8): {to_drop_high_corr}\")\n",
    "\n",
    "    print(f\"Shape after feature engineering: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92387093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(X_train_df, X_test_df):\n",
    "    numerical_features = X_train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_features = X_train_df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train_df)\n",
    "    X_test_transformed = preprocessor.transform(X_test_df)\n",
    "\n",
    "    print(f\"Transformed training data shape: {X_train_transformed.shape}\")\n",
    "    print(f\"Transformed test data shape: {X_test_transformed.shape}\")\n",
    "    return X_train_transformed, X_test_transformed, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe2330b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, set_name=\"\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{set_name} evaluation\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"  Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"  R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e34b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, models):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting initial model training and evaluation...\")\n",
    "    print(\"=\"*50)\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nModel: {name}\")\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        evaluate_model(y_test, y_test_pred, \"Test set\")\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        evaluate_model(y_train, y_train_pred, \"Train set\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d8d35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X_train, y_train, tunable_models_params):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting hyperparameter tuning with RandomizedSearchCV...\")\n",
    "    print(\"=\"*50)\n",
    "    model_best_params = {}\n",
    "    for name, model, params in tunable_models_params:\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "        random_search = RandomizedSearchCV(model, params, cv=5, n_iter=10, n_jobs=-1, random_state=42, verbose=1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_params = random_search.best_params_\n",
    "        model_best_params[name] = best_params\n",
    "        print(f\"Best parameters for {name}: {best_params}\")\n",
    "    return model_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f85ab00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_models_and_predict(X_train, y_train, X_test_processed, test_ids, best_params_dict):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training best models and generating predictions...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    best_xgboost_model = XGBRegressor(\n",
    "        max_depth=best_params_dict.get('XG-boost', {}).get('max_depth', 5),\n",
    "        learning_rate=best_params_dict.get('XG-boost', {}).get('learning_rate', 0.01),\n",
    "        n_estimators=best_params_dict.get('XG-boost', {}).get('n_estimators', 300),\n",
    "        gamma=best_params_dict.get('XG-boost', {}).get('gamma', 0.1),\n",
    "        subsample=best_params_dict.get('XG-boost', {}).get('subsample', 0.6),\n",
    "        colsample_bytree=best_params_dict.get('XG-boost', {}).get('colsample_bytree', 0.8),\n",
    "        reg_alpha=best_params_dict.get('XG-boost', {}).get('reg_alpha', 0.1),\n",
    "        reg_lambda=best_params_dict.get('XG-boost', {}).get('reg_lambda', 1.5),\n",
    "        n_jobs=-1,\n",
    "        min_child_weight=best_params_dict.get('XG-boost', {}).get('min_child_weight', 5)\n",
    "    )\n",
    "\n",
    "    models_to_predict = {\n",
    "        'XG-boost': best_xgboost_model,\n",
    "    }\n",
    "\n",
    "    for name, model in models_to_predict.items():\n",
    "        print(f\"Fitting {name} model for final prediction...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test_processed)\n",
    "\n",
    "        output = pd.DataFrame({\n",
    "            'Id': test_ids,\n",
    "            'SalePrice': y_pred\n",
    "        })\n",
    "        output_filename = f'{name.replace(\" \", \"_\").lower()}_submission.csv'\n",
    "        output.to_csv(output_filename, index=False)\n",
    "        print(f\"âœ… Predictions for {name} saved as '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8af260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns with more than 600 null values: ['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n",
      "Shape after dropping high-null columns: (1460, 74)\n",
      "Missing values in numerical and categorical columns handled.\n",
      "Dropped columns with more than 600 null values: ['Alley', 'MasVnrType', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n",
      "Shape after dropping high-null columns: (1459, 73)\n",
      "Missing values in numerical and categorical columns handled.\n",
      "Dropped columns unique to train set: ['Id']\n",
      "Train-validation split done. X_train_split shape: (1095, 73)\n",
      "Transformed training data shape: (1095, 225)\n",
      "Transformed test data shape: (365, 225)\n",
      "\n",
      "==================================================\n",
      "Starting initial model training and evaluation...\n",
      "==================================================\n",
      "\n",
      "Model: Random Forest\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 17094.4650\n",
      "  Mean Squared Error (MSE): 766765762.8639\n",
      "  Root Mean Squared Error (RMSE): 27690.5356\n",
      "  R2 Score: 0.8905\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 6700.0333\n",
      "  Mean Squared Error (MSE): 144282577.9208\n",
      "  Root Mean Squared Error (RMSE): 12011.7683\n",
      "  R2 Score: 0.9762\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Linear Regression\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 18869.2790\n",
      "  Mean Squared Error (MSE): 2198860315.1492\n",
      "  Root Mean Squared Error (RMSE): 46892.0069\n",
      "  R2 Score: 0.6861\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 13177.9389\n",
      "  Mean Squared Error (MSE): 410076466.2049\n",
      "  Root Mean Squared Error (RMSE): 20250.3448\n",
      "  R2 Score: 0.9325\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Ridge Regression\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 19179.0572\n",
      "  Mean Squared Error (MSE): 827499621.3923\n",
      "  Root Mean Squared Error (RMSE): 28766.2931\n",
      "  R2 Score: 0.8819\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 15121.9355\n",
      "  Mean Squared Error (MSE): 570885855.3480\n",
      "  Root Mean Squared Error (RMSE): 23893.2178\n",
      "  R2 Score: 0.9060\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Lasso Regression\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 18632.0662\n",
      "  Mean Squared Error (MSE): 2081308301.6638\n",
      "  Root Mean Squared Error (RMSE): 45621.3580\n",
      "  R2 Score: 0.7029\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 13204.2475\n",
      "  Mean Squared Error (MSE): 410699419.0546\n",
      "  Root Mean Squared Error (RMSE): 20265.7203\n",
      "  R2 Score: 0.9324\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Decision Tree\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 26468.7178\n",
      "  Mean Squared Error (MSE): 1460958082.2685\n",
      "  Root Mean Squared Error (RMSE): 38222.4814\n",
      "  R2 Score: 0.7914\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 0.0000\n",
      "  Mean Squared Error (MSE): 0.0000\n",
      "  Root Mean Squared Error (RMSE): 0.0000\n",
      "  R2 Score: 1.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Support Vector Regression\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 57168.5907\n",
      "  Mean Squared Error (MSE): 7181711917.1123\n",
      "  Root Mean Squared Error (RMSE): 84744.9817\n",
      "  R2 Score: -0.0252\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 54951.7390\n",
      "  Mean Squared Error (MSE): 6344080944.7726\n",
      "  Root Mean Squared Error (RMSE): 79649.7391\n",
      "  R2 Score: -0.0449\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 20371.4921\n",
      "  Mean Squared Error (MSE): 1260422117.4576\n",
      "  Root Mean Squared Error (RMSE): 35502.4241\n",
      "  R2 Score: 0.8201\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 18496.7753\n",
      "  Mean Squared Error (MSE): 1078233941.2064\n",
      "  Root Mean Squared Error (RMSE): 32836.4727\n",
      "  R2 Score: 0.8224\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: XG-boost\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 17223.6855\n",
      "  Mean Squared Error (MSE): 710355456.0000\n",
      "  Root Mean Squared Error (RMSE): 26652.4944\n",
      "  R2 Score: 0.8986\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 820.2269\n",
      "  Mean Squared Error (MSE): 1366672.0000\n",
      "  Root Mean Squared Error (RMSE): 1169.0475\n",
      "  R2 Score: 0.9998\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 16735.9112\n",
      "  Mean Squared Error (MSE): 664107521.4245\n",
      "  Root Mean Squared Error (RMSE): 25770.2837\n",
      "  R2 Score: 0.9052\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 10202.2946\n",
      "  Mean Squared Error (MSE): 196765122.4415\n",
      "  Root Mean Squared Error (RMSE): 14027.2992\n",
      "  R2 Score: 0.9676\n",
      "--------------------------------------------------\n",
      "\n",
      "Model: Ada Boosting\n",
      "Test set evaluation\n",
      "  Mean Absolute Error (MAE): 24468.8033\n",
      "  Mean Squared Error (MSE): 1134973138.0178\n",
      "  Root Mean Squared Error (RMSE): 33689.3624\n",
      "  R2 Score: 0.8380\n",
      "Train set evaluation\n",
      "  Mean Absolute Error (MAE): 21773.0503\n",
      "  Mean Squared Error (MSE): 764021268.5111\n",
      "  Root Mean Squared Error (RMSE): 27640.9347\n",
      "  R2 Score: 0.8742\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Starting hyperparameter tuning with RandomizedSearchCV...\n",
      "==================================================\n",
      "\n",
      "Tuning Gradient Boosting...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best parameters for Gradient Boosting: {'subsample': 1.0, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 6, 'learning_rate': 0.05}\n",
      "\n",
      "Tuning Random Forest...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df_train, df_final_test_raw = load_data()\n",
    "\n",
    "    final_test_ids = df_final_test_raw['Id']\n",
    "\n",
    "    y_train_target = df_train['SalePrice']\n",
    "    X_train_features = df_train.drop(['SalePrice'], axis=1)\n",
    "\n",
    "    X_final_test_features = df_final_test_raw.drop(['Id'], axis=1)\n",
    "\n",
    "    X_train_features = handle_missing_values(X_train_features.copy())\n",
    "    X_final_test_features = handle_missing_values(X_final_test_features.copy())\n",
    "\n",
    "    train_cols = X_train_features.columns\n",
    "    test_cols = X_final_test_features.columns\n",
    "\n",
    "    unique_to_train = set(train_cols) - set(test_cols)\n",
    "    unique_to_test = set(test_cols) - set(train_cols)\n",
    "\n",
    "    if unique_to_train:\n",
    "        X_train_features = X_train_features.drop(columns=list(unique_to_train))\n",
    "        print(f\"Dropped columns unique to train set: {list(unique_to_train)}\")\n",
    "    if unique_to_test:\n",
    "        X_final_test_features = X_final_test_features.drop(columns=list(unique_to_test))\n",
    "        print(f\"Dropped columns unique to test set: {list(unique_to_test)}\")\n",
    "\n",
    "    X_final_test_features = X_final_test_features[X_train_features.columns]\n",
    "\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train_features, y_train_target, test_size=0.25, random_state=42\n",
    "    )\n",
    "    print(f\"Train-validation split done. X_train_split shape: {X_train_split.shape}\")\n",
    "\n",
    "    X_train_processed, X_val_processed, preprocessor_fitted = preprocess_features(X_train_split, X_val_split)\n",
    "    X_final_test_processed = preprocessor_fitted.transform(X_final_test_features)\n",
    "\n",
    "    initial_models = {\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(random_state=42),\n",
    "        'Lasso Regression': Lasso(random_state=42),\n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "        'Support Vector Regression': SVR(),\n",
    "        'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "        'XG-boost': XGBRegressor(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'Ada Boosting': AdaBoostRegressor(random_state=42)\n",
    "    }\n",
    "\n",
    "    train_and_evaluate_models(X_train_processed, y_train_split, X_val_processed, y_val_split, initial_models)\n",
    "\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    gb_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'subsample': [0.7, 0.8, 1.0],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    xgb_params = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, 10],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'reg_alpha': [0, 0.1, 1],\n",
    "        'reg_lambda': [1, 1.5, 2]\n",
    "    }\n",
    "\n",
    "    tunable_models_params = [\n",
    "        ('Gradient Boosting', GradientBoostingRegressor(random_state=42), gb_params),\n",
    "        ('Random Forest', RandomForestRegressor(random_state=42), rf_params),\n",
    "        ('XG-boost', XGBRegressor(random_state=42), xgb_params)\n",
    "    ]\n",
    "\n",
    "    best_params = tune_hyperparameters(X_train_processed, y_train_split, tunable_models_params)\n",
    "\n",
    "    train_best_models_and_predict(X_train_processed, y_train_split, X_final_test_processed, final_test_ids, best_params)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
